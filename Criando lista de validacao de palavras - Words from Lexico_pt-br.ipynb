{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cea9a08",
   "metadata": {},
   "source": [
    "Este notebook teve como objetivo ler uma lista de palavras lexporbr_alfa_excel.xlsx, filtrar de uma forma completamente manual, retornando um json que é utilizado pelo jogo imbroglio.com.br para VALIDAR se uma palavra existe ou não.\n",
    "\n",
    "Outros notebooks como \"conjugando verbos.ipynb\" e \"Limpando JSON.ipynb\" complementam a saida desse notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801459f0",
   "metadata": {},
   "source": [
    "### Instruções para interpretação do arquivo lexporbr_alfa_excel.xlsx\n",
    "\n",
    "Segue abaixo o nome, a abreviação e a descrição das 21 colunas de informações apresentadas\n",
    "nos resultados de uma pesquisa no Léxico do Português Brasileiro, versão Alfa.\n",
    "Ortografia (orto): forma ortográfica da palavra em letras minúsculas (com exceção dos\n",
    "nomes próprios), respeitando os acentos específicos de cada palavra37\n",
    ".\n",
    "- Categoria gramatical (cat_gram):<\n",
    "    - categorial gramatical da palavra (adj, adv, gram, nom,num, prop, ver).\n",
    "\n",
    "- Informação gramatical (inf_gram): \n",
    "    - informações gramaticais sobre a palavra (ex. singular/plural, masculino/feminino, passado/presente/futuro, 1/2/3 pessoas, etc.). \n",
    "- Frequência ortográfica (freq_orto): \n",
    "    - número de vezes que a palavra aparece no NILC (cerca de 32 milhões de palavras).\n",
    "\n",
    "- Frequência ortográfica por milhão (freq_orto/M): \n",
    "     - número de vezes que a palavra aparece entre 1 milhão de palavras. Valor padrão para frequência de palavras.\n",
    "\n",
    "- Logaritmo natural da frequência ortográfica (log10_freq_orto): \n",
    "    - logarítmico natural da frequência ortográfica. Os valores logarítmicos são utilizados para linearizar-se o comportamento das frequências das palavras no corpus.\n",
    "\n",
    "- Número de letras (nb_letras):\n",
    "    - número de letras da palavra.\n",
    "\n",
    "- Número de homógrafas (nb_homogr):\n",
    "    - número de palavras homógrafas. Palavras que possuem a mesma ortografia ou diferenças de acentos, mas pertencem a categorias gramaticais diferentes.\n",
    "\n",
    "- Homógrafas (homografas): \n",
    "     - categorias gramaticais das palavras homógrafas.\n",
    "\n",
    "- Ponto de unicidade ortográfico (pu_orto): \n",
    "    - letra a partir da qual a palavra se dissocia das outras, ou seja, letra a partir da qual a palavra é única. Sentido da esquerda para direita.\n",
    "\n",
    "- Vizinhos ortográficos (viz_orto): \n",
    "     - número de vizinhos ortográficos a partir do N de Coltheart, ou seja, alterando-se apenas uma letra por vez (Coltheart et al., 1977).\n",
    "\n",
    "- Distância de Leveinshtein ortográfica (old20):\n",
    "     - distância ortográfica de Leveinshtein das 20 palavras mais póximas calculadas a partir de regressões lineares (Yarkoni et al., 2008).\n",
    "\n",
    "- Estrutura CVCV (CVCV_orto):\n",
    "    - estrutura CVCV da palavra, onde consoantes são C e vogais são V. Ainda, A para acentos, P para pontuação, N para números e S para símbolos.\n",
    "\n",
    "- Bigramas (bigramas): \n",
    "    - bigramas que constituem a palavra separados por “_” e limitados por “#”. O número de bigramas é igual ao número de letras da palavra mais 1.\n",
    "\n",
    "- Trigramas (trigramas):\n",
    "     - trigramas que constituem a palavra separados por “_” e limitados por “#”. O número de trigramas é igual ao número de letras da palavra.\n",
    "\n",
    "- Ortografia invertida (inv_orto):\n",
    "    - forma invertida da ortografia (orto).\n",
    "\n",
    "- Estrutura CVCV invertida (inv_CVCV_orto):\n",
    "    - estrutura CVCV da palavra invertida a partir de (CVCV_orto).\n",
    "\n",
    "- Bigramas invertidos (inv_bigra): \n",
    "     - bigramas que constituem a palavra separados por “_” e limitados por “#” invertidos a partir de (bigramas).\n",
    "\n",
    "- Trigramas invertidos (inv_trigra):\n",
    "    - trigramas que constituem a palavra separados por “_” e limitados por “#” invertidos a partir de (trigramas).\n",
    "\n",
    "- Número aleatório entre 0 e 1 (aleatorio): \n",
    "    - número aleatório entre 0 e 1 com oito algarismos de precisão.\n",
    "\n",
    "- Número de identificação (id): \n",
    "    - número de identificação da palavra designado a partir da organização do corpus por frequência decrescente e ordem alfabética a-z. O número de identificação é a posição da palavra no corpus e no léxico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050151a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6738b",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                    if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratativa(X):  \n",
    "    '''\n",
    "    Entrada: series\n",
    "    X_norm \n",
    "        - retira acentos das palavras \n",
    " \n",
    "    list comprehension\n",
    "        - remove palavras com hifen\n",
    "        - remove \".\"\n",
    "        - remove palavra com menos de 2 caracter\n",
    "        - remove palavras com letras maisculas (nomes proprios)\n",
    "        - remove palavras com caracteres especiais utilizando regex\n",
    "        - retira palavras escolhidas\n",
    "        \n",
    "    return: 2 listas, 1 de palavras sem acentos e outra normal\n",
    "    '''\n",
    "\n",
    "    \n",
    "    #verificacao do regex para caracteres especiais\n",
    "    string_check = re.compile(\"[ƒµ‚†„\\ˆ¥÷œ‡þ»+'-@_!#$%^&*()<>?/\\|}{~:ºª°º¹²³£¢¬]\")\n",
    "\n",
    "    word_list = [x for x in X\n",
    "                if  '-' not in x \n",
    "                and string_check.search(x) == None\n",
    "                #and len(x) > 2 \n",
    "                and '.' not in x \n",
    "                and x.lower() == x\n",
    "                ]\n",
    "\n",
    "    \n",
    "    word_list_clean = [x for x in word_list if x not in palavroes and x not in norm_palavroes]\n",
    "        \n",
    "    \n",
    "    word_list_clean = [x for x in word_list_clean \n",
    "                         if ''.join(\n",
    "                                 [letter for letter in x \n",
    "                                      if not letter.isdigit()]\n",
    "                                 )\n",
    "                                ]\n",
    "    \n",
    "    word_list_pt = [x for x in word_list_clean if x not in only_en]\n",
    "    \n",
    "    # removendo acentos\n",
    "    y = pd.Series(word_list_pt)\n",
    "    X_norm = y.apply(lambda row: strip_accents(row))\n",
    "    \n",
    "    return X_norm.to_list(), word_list_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e51d13",
   "metadata": {},
   "source": [
    "# IMPORTANDO DICIONARIO PORTUGUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Data/dicionario/lexporbr_alfa_excel.xlsx'\n",
    "lexico = pd.read_excel(PATH)\n",
    "lexico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45702be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico[lexico.cat_gram == 'ver'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664feae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66bab063",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico.ortografia.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico.ortografia.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264f97a",
   "metadata": {},
   "source": [
    "# Iniciando Tratativas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d1e3a",
   "metadata": {},
   "source": [
    "### NULOS E DUPLICADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3140376",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico = lexico.dropna(subset=['ortografia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c84dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexico = lexico.drop_duplicates(subset= 'ortografia', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288d33a",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24598a3b",
   "metadata": {},
   "source": [
    "## QUANTIDADE DE LETRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd589e",
   "metadata": {},
   "source": [
    "#### cria dataframe com todas as palavras maiores que 2 caracteres que não sejam numericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85851f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico_filtered = lexico[(lexico['nb_letras'] > 2) & (lexico.cat_gram != 'num')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bb084",
   "metadata": {},
   "source": [
    "##### cria dataframe com todas as palavras de 2 caracteres que não sejam numericos para futuruas tratativas diferenciadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico_filtered_2_letras = lexico[(lexico['nb_letras'] == 2) & (lexico.cat_gram != 'num')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc059ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico_filtered_2_letras = lexico_filtered_2_letras.sort_values(by='freq_orto/M', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84492631",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico_filtered_2_letras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico_filtered_2_letras.head()['ortografia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be6099",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46607a",
   "metadata": {},
   "source": [
    "# LISTA PALAVRAS EM INGLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd291428",
   "metadata": {},
   "source": [
    "#####  lendo dicionario lexico em ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://crr.ugent.be/programs-data/subtitle-frequencies\n",
    "lexico_en = pd.read_excel(r'Data\\dicionario\\SUBTLEXusfrequencyabove1.xls')\n",
    "lexico_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c838b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico_en.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d19b56",
   "metadata": {},
   "source": [
    "##### transformando em lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2471b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = lexico_en.Word.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea31f31b",
   "metadata": {},
   "source": [
    "##### ordenando pelas mais frequentes (as palavras em pt br ficarão por ultimo, na 6077 colocação) - leitura visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_en = lexico[lexico.ortografia.isin(english_words)].sort_values(by='freq_orto/M').ortografia.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_en[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527decf",
   "metadata": {},
   "source": [
    "##### apos verificação manual, notei que essas palavras são brasileiras (ou frequentemente utilizadas no brasil) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f35484",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_br ='''exotica, formulas, scan, magazine, erotica, level, karaoke, guacamole, hospital, bug, bugs, cadaver, câncer, videocassete, vina, vodka, trial, loop, loops, protons, pet, fã, delete, kilo, kilos, cameras, camera, supernatural, short, use, megabytes, polyester, tsunami, médico, live, pátio, inglês, vírus, álbum, mimosa, língua, bio, zap, premium, aliens, alien, pet, pets, hétero, áudio, nerds, nerd, panda, pandas, ninja, ninjas, revise, desktop, anime, cloaca, insular, retro, revolver, indie, neural, macular, medicina, multi, multicultural, auras, aura, container, containers, whisky, whiskies, torso, tiara, ultimo, tutu, local, radio, debutante, flamenco, boxer  , box, combine, cacam baste, basta, barmen, ancora, abates, impostor, iguana, zona, senhorita, led, megawatt, familia, arenas, arena, avance, melodrama, network, poodle, script, semi, juris, jambo, ignore, hacker, gurus, cameraman, avatar, motocross, moto, snowboard, placebo, hobbies, hobby, colas, celestial, celeste, cardiovascular, interface, interfaces, cola, elixir, elixires, vulva, tech, taverna, superego, paranormal, tavernas, portifólio, fondue, escudos, clerical, mandala, manda, número, console, jeep, tempera, silos, poster, pixel, pixels, remix, clique, bike, bikes, cliques, samurais, samurai, sonata, banda, bandas, neuroses, peques, rumba, torpedo, tutorial, modular, consular, carte, intima, intimo, intimas, intimos, utublar, tender, sequela, patina, máscara, iguanas, hangar, ferias, edema, descamisados, jumbo, floral, latitude, gourmet, piano, pianos, marrom, marrons, lambada, fax, faxes, surreal, solicitude, critique, date, examine, replay, remove, vendor, mica, pesetas, dolar, tótem, proverbial, fraternal, nasal, intercontinental, paternal, maternal, feedback, crepe, clone, clones, converse, suíte, merengue, ornamental, sapiens, laptop, lustre, ballet, force, cowboy, cetera, bodes, batom, batons, muleta, tango, pulsar, plebe, mambo, pata, garagem, torque, zoom, subverter, facial, larva, margarita, vascular, servo, virtuoso, presto, predecessor, mole, moles, cache, decimal, declare, capô, temperamental, piranha, piranhas, flamingos, abdominal, cabanas, dons, egos, vertebral, posteriori, recuse, diocese, triangular, radial, pólo, mamas, vesceral, eras, manifestos, manifesto, novena, neutrino, colossal, zebras, zebra, doer, ketchup, mini, posses, posse, vagina, menstrual, menstrua, complete, internet, perpendicular, reserve, marque, libido, tacos, taco, magma, umbilical, burros, jingles, provincial, provincia, shop, sensor, poses, fosse, gospel, causa, hematomas, pampas, sbsolve, chef, chefs, exportar, picante, media, aficionados, vans, van, gringo, infame, ocular, salvos, resorte,transistor, tribal, pedestal, anorexia, cone, cones, drinks, drink, pastoral, portal, fórmula, servos, longitudinal, carnal, frame, chassi, visor, limbo, mucosa, aromas, aroma, renal, volts, fetal, homo, concha, genital, spray, hormonal, anal, dorsal, rifle, paparazzi, menu, casco, caviar, manias, salve, fado, lira, haste, logo, logos, expert, loco, loca, compressor, trailer, longitude, sina, imbórglio, grata, altitudes, orbital, rim, skate, champagne, detector, prepare, angular, fractal, executor, modelo, tutor, tapas, video, papas, papa, nuances, manicure, gringos, feudal, você, lava, rea, televisor, mosquitos, mosquito, vocês, feudais, lavas, machismo, trombone, tequila, revive, soda, ardor, enigmas, enigma, sushi, molar, rata, rato, precede, cover, dental, baby, bolos, note, diagonal, salsa, capacitor, odor, secular, pedal, cia, apologia, realize'''\n",
    "list_br = list_br.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb891f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a10923",
   "metadata": {},
   "source": [
    "##### Apos a 6077 palavra, a maioria das palavras estão em portugues, então criei uma lista pra retirar as de ingles entre elas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_en = '''hip, flat, club, jet, must, killer, blazer, it, est, kitsch, heavy, new, royalties, remake, you, rockets, slides, he, business, ops, thriller, notes, made, is, hall, es, by, overnight, country, playoffs, franchising, du, th, cr, soft, food, free, drag, out, soccer, at, cap, star, big, flash, rpm, point, notes, seller, best, zapping, ita, ad, off, qua, quas, es, made, is, mote, line, con, grid, besides, look, pool, s, ex, prime, ohms, ram, game, rg, fr, op, cc, ante, on, to, per, et, en, light, y, sex, n, bi, over, of, di, cm, mi, t, in, kg, iii, te, d, 0'''\n",
    "list_en = list_en.split(', ')\n",
    "list_en = pd.Series(list_en).apply(lambda row: strip_accents(row))\n",
    "list_en = list_en.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a3128",
   "metadata": {},
   "source": [
    "#### verifica se as primeiras 6077 palavras da lista de palavras \"em ingles\" não estão na lista de palavras em br e as proxima ESTÃO  lista de palavras ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab555a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retirando as br da lista de ingles\n",
    "only_en = [x for x in br_en[0:6077] if x not in list_br] + [y for y in br_en[6077:] if y in list_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac153078",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_en[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70056f0b",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf070b6",
   "metadata": {},
   "source": [
    "# PALAVRÕES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf33f1",
   "metadata": {},
   "source": [
    "#### lendo o ARQUIVO DE PALAVROES \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lendo o ARQUIVO DE PALAVROES \n",
    "f = open('Data/palavroes.txt') \n",
    "palavroes = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavroes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19fb51",
   "metadata": {},
   "source": [
    "#### Tratando palavrões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f683c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavroes = pd.Series(palavroes).apply(lambda x: x.lower().replace('\\n ', '').replace(' ', ''))\n",
    "norm_palavroes = pd.Series(palavroes).apply(lambda row: strip_accents(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f65f9c",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be449c6",
   "metadata": {},
   "source": [
    "# CHAMADA DAS FUNÇÕES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word_list, word_list= tratativa(lexico_filtered['ortografia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word_list_2_letras, word_list_2_letras= tratativa(lexico_filtered_2_letras['ortografia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639dc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word_list_2_letras[0:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "'jet' in word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8235f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988d846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83e7bf39",
   "metadata": {},
   "source": [
    "# Adicionando palavras com 2 letras na lista de palavras totais (normalizadas e nao normalizadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apos feito uma inspeção visual, foi defido que das 478 palavras de 2 letras, apenas as 32 primeiras sao relevantes. \n",
    "#Mesmo com alguns lixos (lista retirar) e de todass as 478 palavras, algumas ainda seriam relevantes (lista inserir e norm_inserir)\n",
    "retirar = ['ao', 'km', 'ii', 'mg']\n",
    "inserir = ['lá', 'fé','tú', 'dê', 'se', 'nó', 'dj', 'br', 'pá', 'dó', 'ok','oi', 'nú']\n",
    "norm_inserir = ['la', 'fe','tu', 'de', 'se', 'no', 'dj', 'br', 'pa', 'do', 'ok','oi', 'nu']\n",
    "\n",
    "\n",
    "# as 32 primeiras palavras da lista estão ok\n",
    "#retirnando as palavras normalizadas escolhidas dentre as 32 primeiras da lista de palavras com 2 letras\n",
    "normalized_word_list_2_letras = [x for x in normalized_word_list_2_letras[0:32] if x not in retirar] \n",
    "\n",
    "#inserindo as palavras normalizadas escolhidas\n",
    "normalized_word_list_2_letras = normalized_word_list_2_letras + norm_inserir\n",
    "\n",
    "#somando com a lista de palavras normalizadas todais\n",
    "normalized_word_list = normalized_word_list + normalized_word_list_2_letras\n",
    "\n",
    "\n",
    "#repetindo o processo para lista de palavras nao normalizadas\n",
    "#retirnando as palavras escolhidas dentre as 32 primeiras da lista de palavras com 2 letras dentre as 32 primeiras\n",
    "word_list_2_letras = [x for x in word_list_2_letras[0:32] if x not in retirar] \n",
    "\n",
    "#inserindo as palavras escolhidas\n",
    "word_list_2_letras = word_list_2_letras + norm_inserir\n",
    "\n",
    "#somando com a lista de palavras\n",
    "word_list = word_list + word_list_2_letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cba62",
   "metadata": {},
   "source": [
    "# CRIANDO O JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3846f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "i = 0\n",
    "dic_aux = {}\n",
    "list_aux = []\n",
    "\n",
    "for word, norm_word in zip(word_list, normalized_word_list):\n",
    "    dic_aux = {\n",
    "        'id': i,\n",
    "        'word':word,\n",
    "        'normalized': norm_word\n",
    "    }    \n",
    "    list_aux.append(dic_aux)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339627d4",
   "metadata": {},
   "source": [
    "## INSERINDO LISTA DE NUMERAIS DIRETO NO JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49baafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerais = 'um, dois, três, quatro, cinco, seis, sete, oito, nove, dez, onze, doze, treze, quatorze, quinze, dezesseis, dezessete, dezoito, dezenove, vinte, trinta, quarenta, cinquenta, sessenta, setenta, oitenta, noventa, cem, duzentos, trezentos, quatrocentos, quinhentos, seiscentos, setecentos, oitocentos, novecentos, mil'\n",
    "numerais = numerais.split(', ')\n",
    "norm_numerais = pd.Series(numerais).apply(lambda row: strip_accents(row))\n",
    "\n",
    "\n",
    "list_aux2 = []\n",
    "for i in range (len(list_aux), len(list_aux) +  len(numerais)):\n",
    "    dic_aux = {\n",
    "        'id': i,\n",
    "        'word': numerais[i - len(list_aux)],\n",
    "        'normalized' : norm_numerais.to_list()[i - len(list_aux)]        \n",
    "    }\n",
    "    \n",
    "    list_aux2.append(dic_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73238340",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_aux = list_aux + list_aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065409e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(list_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604c581",
   "metadata": {},
   "source": [
    "## SALVANDO O JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1dca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvando o arquivo json\n",
    "#with open('Data/words.json', 'w') as f:\n",
    "#    json.dump(list_aux, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375ddd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2abbb327",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c786718",
   "metadata": {},
   "source": [
    "# RESULTADOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd702b",
   "metadata": {},
   "source": [
    "#### LENDO O JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lendo o JSON \n",
    "f = open('Data/words.json') \n",
    "words_file = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "normalized_word_list =[]\n",
    "\n",
    "for dic in words_file:\n",
    "    word_list.append(dic['word'])\n",
    "    normalized_word_list.append(dic['normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'word': word_list,\n",
    "      'norm_word': normalized_word_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(dic, columns =['word', 'norm_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7be533",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb8746",
   "metadata": {},
   "source": [
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
